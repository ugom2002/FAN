{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training the Classic Transformer ===\n"
     ]
    }
   ],
   "source": [
    "# Transformer_FAN_vs_Classic.ipynb\n",
    "\n",
    "# In[1]: Import libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_loader import load_etth1\n",
    "from models import TransformerForecast\n",
    "\n",
    "# In[2]: Define training and evaluation functions\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)  # x: (batch, seq_len, input_size)\n",
    "            y = y.to(device)  # y: (batch, pred_len, input_size)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)  # outputs: (batch, pred_len, input_size)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "        losses.append(epoch_loss)\n",
    "    return losses\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    print(f'Test Loss: {avg_loss:.4f}')\n",
    "    return avg_loss\n",
    "\n",
    "# In[3]: Parameters and data loading\n",
    "seq_len = 96           # Length of the input sequence\n",
    "pred_len = 24          # Length of the prediction window\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "d_model = 64\n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "d_ff = 128\n",
    "input_size = 7         # Multivariate: for example, 7 variables (HUFL, HULL, MUFL, MULL, LUFL, LULL, OT)\n",
    "\n",
    "# Ensure that the 'data/ETTh1.csv' file is available and prepared for multivariate input\n",
    "train_dataset, test_dataset = load_etth1('data/ETTh1.csv', seq_len, pred_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# In[4]: Training the Classic Transformer\n",
    "print(\"=== Training the Classic Transformer ===\")\n",
    "model_classic = TransformerForecast(input_size, d_model, nhead, num_layers, d_ff, pred_len, use_fan=False)\n",
    "optimizer_classic = optim.Adam(model_classic.parameters(), lr=learning_rate)\n",
    "losses_classic = train_model(model_classic, train_loader, criterion, optimizer_classic, num_epochs, device)\n",
    "test_loss_classic = evaluate_model(model_classic, test_loader, criterion, device)\n",
    "\n",
    "# Save the Classic Transformer model\n",
    "torch.save(model_classic.state_dict(), 'results/model_classic.pth')\n",
    "print(\"Classic Transformer model saved as 'model_classic.pth'.\")\n",
    "\n",
    "\n",
    "# In[5]: Training the Transformer with FAN\n",
    "print(\"=== Training the Transformer with FAN ===\")\n",
    "model_fan = TransformerForecast(input_size, d_model, nhead, num_layers, d_ff, pred_len, use_fan=True, fan_p_ratio=0.25)\n",
    "optimizer_fan = optim.Adam(model_fan.parameters(), lr=learning_rate)\n",
    "losses_fan = train_model(model_fan, train_loader, criterion, optimizer_fan, num_epochs, device)\n",
    "test_loss_fan = evaluate_model(model_fan, test_loader, criterion, device)\n",
    "\n",
    "# Save the Transformer with FAN model\n",
    "torch.save(model_fan.state_dict(), 'results/model_fan.pth')\n",
    "print(\"Transformer with FAN model saved as 'model_fan.pth'.\")\n",
    "\n",
    "# In[6]: Compare training loss curves\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(losses_classic, label='Classic Transformer')\n",
    "plt.plot(losses_fan, label='Transformer with FAN')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Comparison of Training Losses\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "seq_len = 96\n",
    "pred_len = 24\n",
    "batch_size = 32\n",
    "input_size = 7  # Multivariate: HUFL, HULL, MUFL, MULL, LUFL, LULL, OT\n",
    "\n",
    "# Load the test dataset\n",
    "_, test_dataset = load_etth1('data/ETTh1.csv', seq_len, pred_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Batch size 1 for easier plotting\n",
    "\n",
    "# Load the trained models (Classic Transformer and FAN Transformer)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Classic Transformer\n",
    "model_classic = TransformerForecast(input_size, d_model=64, nhead=4, num_layers=2, d_ff=128, pred_len=pred_len, use_fan=False)\n",
    "model_classic.load_state_dict(torch.load('model_classic.pth', map_location=device))  # Replace with your classic model file\n",
    "model_classic.to(device)\n",
    "model_classic.eval()\n",
    "\n",
    "# Load FAN Transformer\n",
    "model_fan = TransformerForecast(input_size, d_model=64, nhead=4, num_layers=2, d_ff=128, pred_len=pred_len, use_fan=True, fan_p_ratio=0.25)\n",
    "model_fan.load_state_dict(torch.load('model_fan.pth', map_location=device))  # Replace with your FAN model file\n",
    "model_fan.to(device)\n",
    "model_fan.eval()\n",
    "\n",
    "# Variable names\n",
    "variables = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n",
    "\n",
    "# Get predictions for the first batch in the test set\n",
    "with torch.no_grad():\n",
    "    for x, y_true in test_loader:\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # Predictions from both models\n",
    "        y_pred_classic = model_classic(x)\n",
    "        y_pred_fan = model_fan(x)\n",
    "        break  # We only need the first batch for plotting\n",
    "\n",
    "# Convert predictions and true values to numpy arrays for plotting\n",
    "y_true = y_true.cpu().numpy()[0]          # Shape: (pred_len, input_size)\n",
    "y_pred_classic = y_pred_classic.cpu().numpy()[0]  # Shape: (pred_len, input_size)\n",
    "y_pred_fan = y_pred_fan.cpu().numpy()[0]          # Shape: (pred_len, input_size)\n",
    "\n",
    "# Plotting real vs predictions from both models for each variable\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 10))  # 3x3 grid for 7 variables (2 empty)\n",
    "fig.suptitle('Real vs Predictions (Classic Transformer vs FAN Transformer)', fontsize=16)\n",
    "\n",
    "for i in range(7):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axs[row, col].plot(y_true[:, i], label='Real', marker='o', color='black')\n",
    "    axs[row, col].plot(y_pred_classic[:, i], label='Classic Transformer', linestyle='--', color='blue')\n",
    "    axs[row, col].plot(y_pred_fan[:, i], label='FAN Transformer', linestyle='-.', color='red')\n",
    "    axs[row, col].set_title(f'Variable: {variables[i]}')\n",
    "    axs[row, col].set_xlabel('Time Step')\n",
    "    axs[row, col].set_ylabel('Normalized Value')\n",
    "    axs[row, col].legend()\n",
    "\n",
    "# Hide the empty subplots\n",
    "for i in range(7, 9):\n",
    "    fig.delaxes(axs.flatten()[i])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit the suptitle\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
